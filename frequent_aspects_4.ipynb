{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "data = pd.read_csv('imdb_dataset.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(4)\n",
    "data1 = data.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adverb + Adjective  + Noun + Noun\n",
    "def aspect_AAjN(txt):\n",
    "    #JJ adjective\n",
    "    #JJR adjective, comparative \n",
    "    #JJS adjective, superlative \n",
    "\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"RB\" or taggedList[i][1]==\"RBR\" or taggedList[i][1]==\"RBS\") and\n",
    "           (taggedList[i+1][1]==\"JJ\") and\n",
    "           (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \"+taggedList[i+1][0]+\" \"+taggedList[i+2][0]])\n",
    "\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AAjN = [aspect_AAjN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "def nested_list_flatten(lis):\n",
    "     for item in lis:\n",
    "            if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                for x in nested_list_flatten(item):\n",
    "                    yield x\n",
    "            else:        \n",
    "                    yield item\n",
    "flat_AAjN = list(nested_list_flatten(categoryList_AAjN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first saw movie', 9), ('doe good job', 8), ('really bad movie', 8), ('billy bob thornton', 8), ('wa great movie', 6), ('pretty much sum', 6), ('quite long time', 6), ('pretty good job', 6), ('never big fan', 6), ('really enjoyed movie', 6), ('obviously low budget', 5), ('pretty much everything', 5), ('genuinely funny moment', 4), ('pretty good movie', 4), ('thrown good measure', 4), ('academy award nomination', 4), ('first fifteen minute', 4), ('well worth effort', 4), ('pretty much nothing', 4), ('well worth time', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter=Counter(flat_AAjN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verb + Noun\n",
    "def aspect_VN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "         if((taggedList[i][1]==\"VB\" or taggedList[i][1]==\"VBP\" or taggedList[i][1]==\"VBG\" or taggedList[i][1]==\"VBN\" or taggedList[i][1]==\"VBD\") and \n",
    "           (taggedList[i+1][1]==\"NN\" or taggedList[i+1][1]==\"NNS\" or taggedList[i+1][1]==\"NNP\" or taggedList[i+1][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_VN= [aspect_VN(data['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_VN = list(nested_list_flatten(categoryList_VN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('take place', 144), ('make sense', 143), ('supporting cast', 87), ('recommend anyone', 78), ('made tv', 61), ('seen movie', 60), ('see movie', 57), ('want see', 55), ('opening scene', 50), ('acting wa', 49), ('watching movie', 48), ('seen film', 47), ('tell story', 47), ('make movie', 46), ('opening credit', 46), ('saving grace', 44), ('supporting role', 42), ('read book', 39), ('want watch', 38), ('love story', 38)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_VN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun + Adverb + Verb \n",
    "def aspect_NAV(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "         if((taggedList[i][1]==\"NN\" or taggedList[i][1]==\"NNS\" or taggedList[i][1]==\"NNP\" or taggedList[i][1]==\"NNPS\")\n",
    "            and (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\")\n",
    "           and (taggedList[i+2][1]==\"VB\" or taggedList[i+2][1]==\"VBP\" or taggedList[i+2][1]==\"VBG\" or taggedList[i+2][1]==\"VBN\" or taggedList[i+2][1]==\"VBD\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_NAV = [aspect_NAV(data['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_NAV = list(nested_list_flatten(categoryList_NAV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie ever seen', 87), ('film ever seen', 51), ('movie ever made', 32), ('film ever made', 21), ('comedy ever seen', 12), ('thing ever seen', 11), ('year ago wa', 9), ('wa really looking', 7), ('wa first released', 7), ('scene well done', 6), ('wa pleasantly surprised', 6), ('wa ever made', 5), ('movie never get', 5), ('anything else seen', 5), ('film well directed', 4), ('ha already said', 4), ('movie first saw', 4), ('thriller ever made', 4), ('movie never seen', 3), ('movie ever watched', 3)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_NAV)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjective + Noun\n",
    "def aspect_AN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if((taggedList[i][1]==\"JJ\" ) and (taggedList[i+1][1]==\"NN\" or taggedList[i+1][1]==\"NNS\" or taggedList[i+1][1]==\"NNP\" or taggedList[i+1][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AN = [aspect_AN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "def nested_list_flatten(lis):\n",
    "     for item in lis:\n",
    "            if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                for x in nested_list_flatten(item):\n",
    "                    yield x\n",
    "            else:        \n",
    "                    yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AN = list(nested_list_flatten(categoryList_AN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('special effect', 441), ('low budget', 304), ('main character', 223), ('new york', 218), ('high school', 169), ('bad guy', 156), ('first time', 152), ('long time', 142), ('real life', 128), ('whole thing', 122), ('many people', 91), ('good movie', 90), ('good thing', 90), ('bad movie', 89), ('good job', 83), ('little bit', 83), ('many time', 78), ('martial art', 77), ('last night', 75), ('young woman', 75)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjective + Noun + Noun\n",
    "def aspect_ANN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"JJ\") and (taggedList[i+1][1]==\"NN\" or taggedList[i+1][1]==\"NNS\" or taggedList[i+1][1]==\"NNP\" or taggedList[i+1][1]==\"NNPS\") and \n",
    "          (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_ANN = [aspect_ANN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ANN = list(nested_list_flatten(categoryList_ANN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new york city', 42), ('complete waste time', 18), ('low budget horror', 15), ('low budget film', 15), ('low budget movie', 14), ('local video store', 12), ('first time saw', 11), ('total waste time', 10), ('high school student', 10), ('cary grant irene', 9), ('bad taste mouth', 8), ('favorite movie time', 8), ('jean claude van', 8), ('right hand man', 7), ('big name actor', 7), ('new york time', 7), ('stop motion animation', 7), ('high production value', 7), ('late night tv', 7), ('high school drama', 7)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_ANN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adverb + Adjective\n",
    "def aspect_AvA(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if((taggedList[i][1]==\"RB\" or taggedList[i][1]==\"RBR\" or taggedList[i][1]==\"RBS\") and \n",
    "           (taggedList[i+1][1]==\"JJ\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AvA = [aspect_AvA(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AvA = list(nested_list_flatten(categoryList_AvA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pretty much', 142), ('pretty good', 118), ('really bad', 83), ('really good', 72), ('quite good', 53), ('definitely worth', 38), ('well worth', 37), ('highly recommend', 37), ('wa good', 36), ('pretty bad', 32), ('completely different', 30), ('still alive', 28), ('almost impossible', 26), ('back forth', 26), ('really want', 25), ('also good', 25), ('late early', 25), ('really much', 25), ('first minute', 24), ('really great', 24)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AvA)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verb + Adjective + Noun\n",
    "def aspect_VAN(txt):\n",
    "    \n",
    "   \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "    \n",
    "    categoryList = []\n",
    "    #VBD\tverb, past tense\ttook\n",
    "    #VBG\tverb, gerund/present participle\ttaking\n",
    "    #VBN\tverb, past participle\ttaken\n",
    "    #VBP\tverb, sing. present, non-3d\ttake\n",
    "    #VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    #NN\tnoun, singular 'desk'\n",
    "    #NNS\tnoun plural\t'desks'\n",
    "    #NNP\tproper noun, singular\t'Harrison'\n",
    "    #NNPS\tproper noun, plural\t'Americans'\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"VB\" or taggedList[i][1]==\"VBP\" or taggedList[i][1]==\"VBD\" or taggedList[i][1]==\"VBG\" or taggedList[i][1]==\"VBN\") and taggedList[i+1][1]==\"JJ\" and (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \"+taggedList[i+1][0]+\" \"+taggedList[i+2][0]])\n",
    "\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_VAN = [aspect_VAN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_VAN = list(nested_list_flatten(categoryList_VAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('seen long time', 32), ('based true story', 26), ('make much sense', 11), ('seen entire life', 9), ('know last summer', 8), ('seen many time', 8), ('acting top notch', 8), ('acting special effect', 7), ('die hard fan', 7), ('based short story', 7), ('make good movie', 7), ('saw first time', 6), ('went straight video', 6), ('done many time', 6), ('done low budget', 6), ('saving private ryan', 6), ('watched several time', 5), ('based true event', 5), ('seeing first time', 5), ('lost new york', 5)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_VAN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjective + verb + Noun\n",
    "def aspect_AVN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"JJ\") and (taggedList[i+1][1]==\"VB\" or taggedList[i+1][1]==\"VBP\" or taggedList[i+1][1]==\"VBG\" or taggedList[i+1][1]==\"VBN\" or taggedList[i+1][1]==\"VBD\") and \n",
    "           (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AVN = [aspect_AVN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AVN = list(nested_list_flatten(categoryList_AVN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural born killer', 7), ('much say film', 5), ('strong supporting cast', 4), ('small supporting role', 4), ('first saw movie', 4), ('excellent supporting cast', 3), ('high paying job', 3), ('many glowing review', 3), ('black gloved killer', 3), ('great supporting cast', 3), ('light hearted comedy', 3), ('bad acting story', 3), ('worth watching see', 2), ('hard believe anyone', 2), ('scary uninspired clone', 2), ('central introduced leader', 2), ('old trying get', 2), ('poor casting choice', 2), ('capable waiting time', 2), ('worth watching recommend', 2)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AVN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjective + Adverb \n",
    "def aspect_AdA(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if((taggedList[i][1]==\"JJ\") and (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AdA = [aspect_AdA(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AdA = list(nested_list_flatten(categoryList_AdA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good enough', 36), ('lucky enough', 23), ('bad enough', 22), ('good well', 22), ('good even', 21), ('bad really', 19), ('single handedly', 17), ('bad even', 15), ('bad actually', 14), ('much better', 13), ('much else', 13), ('good especially', 12), ('bad ugly', 10), ('great really', 10), ('funny even', 10), ('great well', 10), ('strong enough', 9), ('sure enough', 9), ('great especially', 9), ('wa pretty', 9)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AdA )\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adverb, adverb and noun\n",
    "\n",
    "def aspect_AvAvN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"RB\" or taggedList[i][1]==\"RBR\" or taggedList[i][1]==\"RBS\") and (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\") and \n",
    "           (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AvAvN = [aspect_AvAvN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AvAvN = list(nested_list_flatten(categoryList_AvAvN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('much better film', 3), ('fast forward button', 3), ('fast forward scene', 3), ('somewhere along line', 3), ('long enough fill', 2), ('far back movie', 2), ('well first story', 2), ('even bother watch', 2), ('brother obviously plot', 2), ('doe well role', 2), ('happily ever strength', 2), ('long ago tomorrow', 2), ('much better movie', 2), ('much better thing', 2), ('never accuse writer', 1), ('long vain hope', 1), ('probably better reputation', 1), ('sure exactly recommendation', 1), ('well together comedy', 1), ('honestly never chance', 1)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AvAvN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adverb + Verb\n",
    "def aspect_AvV(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if((taggedList[i][1]==\"RB\" or taggedList[i][1]==\"RBR\" or taggedList[i][1]==\"RBS\") and \n",
    "           (taggedList[i+1][1]==\"VB\" or taggedList[i+1][1]==\"VBP\" or taggedList[i+1][1]==\"VBG\" or taggedList[i+1][1]==\"VBN\" or taggedList[i+1][1]==\"VBD\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AvV = [aspect_AvV(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AvV = list(nested_list_flatten(categoryList_AvV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ever seen', 425), ('ever made', 153), ('never seen', 121), ('well done', 118), ('well known', 73), ('well written', 66), ('first saw', 63), ('never get', 62), ('never heard', 62), ('well made', 57), ('highly recommend', 53), ('highly recommended', 52), ('really make', 43), ('really enjoyed', 38), ('really need', 34), ('never see', 34), ('even know', 32), ('ever see', 29), ('never got', 26), ('really care', 26)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AvV )\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verb, Adverb \n",
    "def aspect_VAv(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if((taggedList[i][1]==\"VB\" or taggedList[i][1]==\"VBP\" or taggedList[i][1]==\"VBG\" or taggedList[i][1]==\"VBN\" or taggedList[i][1]==\"VBD\") and \n",
    "           (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_VAv = [aspect_VAv(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_VAv = list(nested_list_flatten(categoryList_VAv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go back', 81), ('looking forward', 63), ('put together', 62), ('thought wa', 62), ('stay away', 60), ('come back', 55), ('take seriously', 47), ('get back', 44), ('get away', 38), ('done better', 36), ('come together', 32), ('running around', 31), ('done well', 31), ('taken seriously', 29), ('let alone', 29), ('make even', 25), ('go far', 25), ('run away', 24), ('come close', 23), ('coming back', 23)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_VAv)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjective + Adverb + Noun\n",
    "def aspect_AjAN(txt):\n",
    "    #JJ adjective\n",
    "    #JJR adjective, comparative \n",
    "    #JJS adjective, superlative \n",
    "\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"JJ\") and \n",
    "           (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\") and \n",
    "           (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \"+taggedList[i+1][0]+\" \"+taggedList[i+2][0]])\n",
    "\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AjAN = [aspect_AjAN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AjAN = list(nested_list_flatten(categoryList_AjAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('top enough approach', 2), ('bad really movie', 2), ('know best kind', 1), ('rigid unintentionally evers', 1), ('funny silly celebrity', 1), ('effective almost quaint', 1), ('available back year', 1), ('many overdone battle', 1), ('putrid slasher comedy', 1), ('summary even child', 1), ('visceral provide sustenance', 1), ('oh art thou', 1), ('shy away anything', 1), ('minimal lonely moment', 1), ('bad ugly film', 1), ('disputatious sometimes kind', 1), ('pit pretty appeal', 1), ('surreal maybe thats', 1), ('vapid nothingness hollywood', 1), ('untrustworthy really source', 1)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AjAN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adverb + Verb +  Noun\n",
    "def aspect_AvVN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"RB\" or taggedList[i][1]==\"RBR\" or taggedList[i][1]==\"RBS\") and \n",
    "           (taggedList[i+1][1]==\"VB\" or taggedList[i+1][1]==\"VBP\" or taggedList[i+1][1]==\"VBG\" or taggedList[i+1][1]==\"VBN\" or taggedList[i+1][1]==\"VBD\") and\n",
    "         (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_AvVN = [aspect_AvVN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_AvVN = list(nested_list_flatten(categoryList_AvVN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ever seen life', 12), ('ever seen plot', 11), ('ever seen story', 10), ('never read book', 8), ('never seen anything', 7), ('first saw film', 7), ('really make sense', 7), ('ever seen movie', 5), ('never heard movie', 5), ('ever seen film', 5), ('never seen movie', 5), ('not believe people', 4), ('well made movie', 4), ('ever seen people', 4), ('really make work', 4), ('first saw movie', 4), ('ever seen thing', 4), ('ever laid eye', 4), ('really want see', 4), ('highly recommend anyone', 4)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_AvVN)\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verb + Adverb +  Noun\n",
    "def aspect_VAvN(txt):\n",
    "    \n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    for line in sentList:\n",
    "        new_txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "    #print(taggedList)\n",
    "    categoryList = []\n",
    "\n",
    "    for i in range(0,len(taggedList)-2):\n",
    "        if((taggedList[i][1]==\"VB\" or taggedList[i][1]==\"VBP\" or taggedList[i][1]==\"VBG\" or taggedList[i][1]==\"VBN\" or taggedList[i][1]==\"VBD\") and \n",
    "           (taggedList[i+1][1]==\"RB\" or taggedList[i+1][1]==\"RBR\" or taggedList[i+1][1]==\"RBS\") and \n",
    "            (taggedList[i+2][1]==\"NN\" or taggedList[i+2][1]==\"NNS\" or taggedList[i+2][1]==\"NNP\" or taggedList[i+2][1]==\"NNPS\")):\n",
    "\n",
    "            categoryList.append([taggedList[i][0]+\" \" +taggedList[i+1][0]+\" \" +taggedList[i+2][0]])\n",
    "        \n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList_VAvN = [aspect_VAvN(data1['unique_sent'][unt]) for unt in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_VAvN = list(nested_list_flatten(categoryList_VAvN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('looking forward movie', 5), ('done better job', 5), ('come back life', 4), ('go back time', 4), ('hit close home', 3), ('stay away film', 3), ('deserved better treatment', 3), ('come back haunt', 3), ('go back home', 3), ('taken seriously rating', 2), ('looking forward film', 2), ('santana sly family', 2), ('shortened still direction', 2), ('make absolutely sense', 2), ('look back time', 2), ('go back see', 2), ('offered absolutely nothing', 2), ('come together end', 2), ('know almost nothing', 2), ('coming back life', 2)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter(flat_VAvN )\n",
    "most_commons = counter.most_common(20)\n",
    "print(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
